{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff2b5d5-26ab-4850-aa0a-81a52d3f031e",
   "metadata": {},
   "source": [
    "# Climate projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2582d7-e68a-4bde-a944-8c494bac342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os \n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import glob \n",
    "\n",
    "# climate-related\n",
    "import pyet\n",
    "import gcsfs\n",
    "import intake\n",
    "import cftime\n",
    "import regionmask\n",
    "from xclim import core \n",
    "from xclim import sdba\n",
    "from xclim import set_options\n",
    "\n",
    "# others\n",
    "import xesmf as xe\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "import beepy as beep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8cb3a-fd12-47e7-8a76-56c444db0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial extent\n",
    "lat_coords = np.arange(-56,-40, 0.5)\n",
    "lon_coords = np.arange(-76,-67, 0.5)\n",
    "\n",
    "# periods \n",
    "baseline_period = slice(\"1985-01-01\", \"2019-12-31\") # ISIMIP3b bias adjustment protocol\n",
    "inter_future_period = slice(\"2020-01-01\", \"2059-12-31\") # Future period to bias correct\n",
    "far_future_period   = slice(\"2060-01-01\", \"2099-12-31\") # Future period to bias correct\n",
    "full_period       = slice(\"1980-01-01\", \"2099-12-31\") # period\n",
    "\n",
    "periods = [baseline_period, inter_future_period, far_future_period]\n",
    "\n",
    "# encoding / chuncks\n",
    "chunks_dict   = {\"lon\": 10, \"lat\": 10, \"time\": -1}\n",
    "encode_t2m    = {'t2m':  {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_tasmin = {'tasmin':  {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_tasmax = {'tasmax':  {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_pr     = {\"pr\": {\"zlib\": True, \"complevel\": 1, \"dtype\": \"float32\"}}\n",
    "encode_pr_alt = {'pr':      {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_pet    = {'pet':     {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_all    = {**encode_tasmin, **encode_tasmax, **encode_pr_alt}\n",
    "\n",
    "# not all variables or scenarios: \n",
    "# ACCESS-CM2, CMCC-CM2-SR5, IITM-ESM, FGOALS-g3\n",
    "\n",
    "# problem member id\n",
    "# BCC-CSM2-MR, \n",
    "\n",
    "# two outputs per ssp\n",
    "# MPI-ESM1-2-HR \n",
    "\n",
    "os.chdir('/home/rooda/Dropbox/')\n",
    "\n",
    "# check what gcm to use (not only generic list) \n",
    "gcm_list  = [\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MIROC6\", \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\"]  \n",
    "\n",
    "ssp_list  = [\"ssp126\", \"ssp585\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53dc91-9e70-4574-921b-7ceee184c7c3",
   "metadata": {},
   "source": [
    "## 1. Download and preprocess selected GCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb6be2-9a88-48d3-9977-d31eecc6b0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_time(ds):\n",
    "    ds = ds.convert_calendar(calendar = 'gregorian', align_on = 'date', missing = np.nan)\n",
    "    ds = ds.sel(time=full_period)\n",
    "    return ds\n",
    "\n",
    "url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "dataframe = intake.open_esm_datastore(url)\n",
    "\n",
    "dataframe = dataframe.search(experiment_id = ['historical'] + ssp_list, # scenarios  \n",
    "                             table_id      = 'day', # time-step\n",
    "                             variable_id   = ['tasmax', 'tasmin',  'pr'], # variables\n",
    "                             source_id     = gcm_list,\n",
    "                             member_id     = \"r1i1p1f1\")  # models\n",
    "\n",
    "kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True}\n",
    "\n",
    "datasets = dataframe.to_dataset_dict(preprocess = fix_time, **kwargs)\n",
    "datasets.keys()\n",
    "beep.beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef4bb5-691e-470b-ae1e-e576d77f95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf469a4-929f-45f4-a20c-9d77d824bc5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):\n",
    "    gcm_historical = next(val for key, val in datasets.items() if gcm + \".historical\" in key)\n",
    "    gcm_historical = gcm_historical.convert_calendar(calendar = 'gregorian', align_on = 'date', missing = np.nan)\n",
    "    gcm_historical = gcm_historical.sel(time = slice(\"1980-01-01\", \"2014-12-31\"))\n",
    "    \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "        gcm_ssp = next(val for key, val in datasets.items() if gcm + \".\" + ssp in key)\n",
    "        gcm_ssp = gcm_ssp.convert_calendar(calendar = 'gregorian', align_on = 'date', missing = np.nan)\n",
    "        \n",
    "        gcm_ssp = xr.concat([gcm_historical, gcm_ssp], dim = \"time\")\n",
    "        gcm_ssp = gcm_ssp.chunk(dict(time=-1, lat = 10, lon = 10))\n",
    "        gcm_ssp = gcm_ssp.interpolate_na(dim=\"time\", method=\"linear\")\n",
    "\n",
    "        gcm_ssp = gcm_ssp.drop([\"height\", \"dcpp_init_year\", \"member_id\", \"time_bounds\"])\n",
    "        gcm_ssp = gcm_ssp.sel(member_id=0, drop=True).sel(dcpp_init_year=0, drop=True)\n",
    "        gcm_ssp.coords['lon'] = (gcm_ssp.coords['lon'] + 180) % 360 - 180\n",
    "        gcm_ssp = gcm_ssp.sortby(gcm_ssp.lon)\n",
    "        gcm_ssp = gcm_ssp.interp(lat = lat_coords, lon = lon_coords)\n",
    "\n",
    "        if 'bnds' in gcm_ssp:\n",
    "            if len(gcm_ssp.bnds.values) > 0:\n",
    "                gcm_ssp = gcm_ssp.sel(bnds = 1, drop = True)\n",
    "        \n",
    "        # change units\n",
    "        gcm_ssp[\"tasmin\"] = gcm_ssp.tasmin - 273.15 # to degC\n",
    "        gcm_ssp[\"tasmax\"] = gcm_ssp.tasmax - 273.15 # to degC\n",
    "        gcm_ssp[\"pr\"]  = (gcm_ssp.pr*84600) # to mm day-1\n",
    "        gcm_ssp.pr.attrs[\"units\"]      = \"mm d-1\"\n",
    "        gcm_ssp.tasmax.attrs[\"units\"]  = \"degC\"\n",
    "        gcm_ssp.tasmin.attrs[\"units\"]  = \"degC\"\n",
    "        gcm_ssp.to_netcdf(\"/home/rooda/Hydro_results/future_raw/\" + gcm + \"_\" + ssp + \".nc\")\n",
    "        beep.beep(1)\n",
    "        \n",
    "beep.beep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dbaba-7872-4f6f-8ecc-c1c7e0f9f389",
   "metadata": {},
   "source": [
    "## 2. Bias correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e1a9b-c730-4544-a6b7-3f31420a863d",
   "metadata": {},
   "source": [
    "## 2.1 Baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d339aa-12b0-4352-b612-2fe810c04d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source data\n",
    "pmet_pp   = xr.open_dataset(\"Patagonia/Data/Zenodo/v11/PP_PMETsim_1980_2020_v11d.nc\",   chunks = chunks_dict)\n",
    "pmet_tmax = xr.open_dataset(\"Patagonia/Data/Zenodo/v11/Tmax_PMETsim_1980_2020_v11d.nc\", chunks = chunks_dict)\n",
    "pmet_tmin = xr.open_dataset(\"Patagonia/Data/Zenodo/v11/Tmin_PMETsim_1980_2020_v11d.nc\", chunks = chunks_dict)\n",
    "\n",
    "regridder = xe.Regridder(pmet_pp, pmet_tmax, \"nearest_s2d\")\n",
    "pmet_pp   = regridder(pmet_pp)\n",
    "\n",
    "pmet_hist = xr.merge([pmet_pp, pmet_tmax, pmet_tmin]).sel(time = baseline_period)\n",
    "pmet_hist = pmet_hist.rename({'longitude': 'lon','latitude': 'lat', 'pp':'pr', 'tmax':'tasmax','tmin':'tasmin'})\n",
    "\n",
    "# subset area\n",
    "shape = gpd.read_file(\"Patagonia/GIS South/Basins_Patagonia_all.shp\")[[\"geometry\"]]\n",
    "shape = shape.buffer(0.20) \n",
    "mask  = regionmask.mask_geopandas(shape, pmet_hist)   >= 0\n",
    "pmet_hist   = pmet_hist.where(mask, drop = True)\n",
    "\n",
    "pmet_hist.pr.attrs[\"units\"]    = \"mm d-1\"\n",
    "pmet_hist.tasmax.attrs[\"units\"]  = \"degC\"\n",
    "pmet_hist.tasmin.attrs[\"units\"]  = \"degC\"\n",
    "\n",
    "pmet_hist[\"pr\"]  = pmet_hist[\"pr\"].astype(\"float32\")\n",
    "pmet_hist = pmet_hist.chunk(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3192ee2-644d-4e4e-a6de-63925955b072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# due to RAM constrains\n",
    "pmet_hist.pr.to_netcdf(\"/home/rooda/Hydro_results/PMETsim_historical_pr.nc\")\n",
    "pmet_hist.tasmax.to_netcdf(\"/home/rooda/Hydro_results/PMETsim_historical_tasmax.nc\")\n",
    "pmet_hist.tasmin.to_netcdf(\"/home/rooda/Hydro_results/PMETsim_historical_tasmin.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e11ca-1f8f-4623-8cc8-f9d720eac06e",
   "metadata": {},
   "source": [
    "## 2.2 Interp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efd417-9f89-4d84-8e36-bf493088a749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load\n",
    "pmet_hist = xr.open_dataset(\"/home/rooda/Hydro_results/PMETsim_historical_tasmax.nc\", chunks = \"auto\")\n",
    "\n",
    "for gcm in tqdm(gcm_list):    \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "        for period in tqdm(periods, leave = False):\n",
    "            \n",
    "            model_ssp = xr.open_dataset(\"/home/rooda/Hydro_results/future_raw/\" + gcm + \"_\" + ssp + \".nc\", chunks = \"auto\")\n",
    "            model_ssp = model_ssp.sel(time  = period)\n",
    "\n",
    "            model_ssp = model_ssp.interp(lat = pmet_hist.lat, lon = pmet_hist.lon,  method = \"linear\", kwargs={\"fill_value\": \"extrapolate\"})\n",
    "            model_ssp = model_ssp.where(pmet_hist.tasmax[0].notnull())\n",
    "            model_ssp = model_ssp.chunk(\"auto\")\n",
    "            model_ssp = model_ssp.astype(\"float32\").unify_chunks()\n",
    "            model_ssp['time'] = model_ssp.indexes['time'].normalize()\n",
    "            \n",
    "            years = str(period)[7:11] + \"_\" + str(period)[21:25]\n",
    "\n",
    "            # save file\n",
    "            model_ssp.tasmax.to_netcdf(\"/home/rooda/Hydro_results/future_interp/TASMAX_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_tasmax)\n",
    "            model_ssp.tasmin.to_netcdf(\"/home/rooda/Hydro_results/future_interp/TASMIN_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_tasmin)           \n",
    "            model_ssp.pr.to_netcdf(\"/home/rooda/Hydro_results/future_interp/PP_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_pr_alt)\n",
    "            model_ssp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bff8be-bb6e-40e8-a6f1-6cef15afc9f7",
   "metadata": {},
   "source": [
    "## 2.3 MBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b681cb5-9f15-4d46-807c-9765422f4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dict = {\"lon\": 50, \"lat\": 50, \"time\": -1}\n",
    "\n",
    "# load\n",
    "pmet_hist_pr     = xr.open_dataset(\"/home/rooda/Hydro_results/PMETsim_historical_pr.nc\", chunks = chunks_dict).pr\n",
    "pmet_hist_tasmax = xr.open_dataset(\"/home/rooda/Hydro_results/PMETsim_historical_tasmax.nc\", chunks = chunks_dict).tasmax\n",
    "pmet_hist_tasmin = xr.open_dataset(\"/home/rooda/Hydro_results/PMETsim_historical_tasmin.nc\", chunks = chunks_dict).tasmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848896a-d797-4d26-a874-39ba09642f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):    \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "        \n",
    "        model_baseline_tasmax = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/TASMAX_\" + gcm + \"_\" + ssp + \"_1985_2019.nc\", chunks = chunks_dict).tasmax\n",
    "        model_baseline_tasmin = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/TASMIN_\" + gcm + \"_\" + ssp + \"_1985_2019.nc\", chunks = chunks_dict).tasmin\n",
    "        model_baseline_pr     = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/PP_\" + gcm + \"_\" + ssp + \"_1985_2019.nc\", chunks = chunks_dict).pr\n",
    "        model_baseline_pr_ad, pth, dP0 = sdba.processing.adapt_freq(pmet_hist_pr, model_baseline_pr, thresh=\"0.1 mm d-1\", group=\"time\")\n",
    "\n",
    "        for period in tqdm(periods[1:3], leave = False):\n",
    "            \n",
    "            years = str(period)[7:11] + \"_\" + str(period)[21:25]\n",
    "\n",
    "            model_future_tasmax = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/TASMAX_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", chunks = chunks_dict).tasmax\n",
    "            model_future_tasmin = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/TASMIN_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", chunks = chunks_dict).tasmin\n",
    "            model_future_pr     = xr.open_dataset(\"/home/rooda/Hydro_results/future_interp/PP_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", chunks = chunks_dict).pr\n",
    "            \n",
    "            # a) Perform an initial univariate adjustment \n",
    "            qdm_tmax = sdba.QuantileDeltaMapping.train(ref = pmet_hist_tasmax,  hist = model_baseline_tasmax,  kind = \"+\", nquantiles=20, group=\"time.month\")\n",
    "            qdm_tmin = sdba.QuantileDeltaMapping.train(ref = pmet_hist_tasmin,  hist = model_baseline_tasmin,  kind = \"+\", nquantiles=20, group=\"time.month\")\n",
    "            qdm_pp   = sdba.QuantileDeltaMapping.train(ref = pmet_hist_pr,      hist = model_baseline_pr_ad,   kind = \"*\", nquantiles=20, group=\"time.month\")\n",
    "\n",
    "            qdm_tmax  = qdm_tmax.adjust(model_future_tasmax, extrapolation=\"constant\", interp=\"nearest\").transpose('time', 'lat', 'lon')\n",
    "            qdm_tmin  = qdm_tmin.adjust(model_future_tasmin, extrapolation=\"constant\", interp=\"nearest\").transpose('time', 'lat', 'lon')\n",
    "            qdm_pp    = qdm_pp.adjust(model_future_pr,       extrapolation=\"constant\", interp=\"nearest\").transpose('time', 'lat', 'lon')\n",
    "\n",
    "            dref      = xr.Dataset(dict(tasmax = pmet_hist_tasmax,      tasmin = pmet_hist_tasmin,      pr = pmet_hist_pr))\n",
    "            scen_hist = xr.Dataset(dict(tasmax = model_baseline_tasmax, tasmin = model_baseline_tasmin, pr = model_baseline_pr_ad))\n",
    "            scen_ssp  = xr.Dataset(dict(tasmax = qdm_tmax,              tasmin = qdm_tmin,              pr = qdm_pp))\n",
    "            #scen_hist[\"time\"] = dref.time # correct date (15 -> 01)\n",
    "\n",
    "            ## b) Stack the variables to multivariate arrays and standardize them\n",
    "            ref   = sdba.processing.stack_variables(dref) # Stack the variables (tasmax and pr)\n",
    "            scenh = sdba.processing.stack_variables(scen_hist)\n",
    "            scens = sdba.processing.stack_variables(scen_ssp)\n",
    "\n",
    "            ref, _, _          = sdba.processing.standardize(ref) # Standardize\n",
    "            allsim, savg, sstd = sdba.processing.standardize(xr.concat((scenh, scens), \"time\"))\n",
    "\n",
    "            hist = allsim.sel(time = scenh.time)\n",
    "            sim  = allsim.sel(time = scens.time)\n",
    "\n",
    "            ## c) Perform the N-dimensional probability density function transform\n",
    "            out = sdba.adjustment.NpdfTransform.adjust(ref, hist, sim, base=sdba.QuantileDeltaMapping, base_kws={\"nquantiles\": 20, \"group\": \"time.month\"}, n_iter=20)  \n",
    "            model_ssp_bc = sdba.processing.reordering(out, scens, group=\"time.month\")\n",
    "            model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "\n",
    "            ## d) Restoring the trend\n",
    "            model_ssp_bc = sdba.processing.reordering(sim, scens, group=\"time\")\n",
    "            model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "            model_ssp_bc = model_ssp_bc.transpose('time', 'lat', 'lon')\n",
    "            model_ssp_bc[\"tasmin\"] = model_ssp_bc.tasmin.where(model_ssp_bc.tasmax > model_ssp_bc.tasmin, model_ssp_bc.tasmax ) # force check Tmax > Tmin\n",
    "            \n",
    "            # save file\n",
    "            model_ssp_bc.tasmax.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/TASMAX_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_tasmax)\n",
    "            model_ssp_bc.tasmin.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/TASMIN_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_tasmin)           \n",
    "            model_ssp_bc.pr.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/PP_\" + gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_pr_alt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcbdea-914c-419f-a195-98387b200cc8",
   "metadata": {},
   "source": [
    "# Potential evaporation (+ average temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba7634-039a-4b70-810a-6adf4babecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):   \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "        for period in tqdm(periods[1:3], leave = False):\n",
    "            \n",
    "            years = str(period)[7:11] + \"_\" + str(period)[21:25]\n",
    "\n",
    "            model_tmax = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/TASMAX_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\", chunks = \"auto\")\n",
    "            model_tmin = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/TASMIN_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\", chunks = \"auto\")\n",
    "            model_tavg = (model_tmax.tasmax + model_tmin.tasmin)/2\n",
    "            model_tavg = model_tavg.rename(\"t2m\").to_dataset()\n",
    "            \n",
    "            lat  = model_tavg.lat * np.pi / 180  \n",
    "\n",
    "            model_pet  = pyet.hargreaves(model_tavg.t2m, model_tmax.tasmax, model_tmin.tasmin, lat = lat, clip_zero = True)\n",
    "            model_pet  = model_pet.rename(\"pet\")\n",
    "            \n",
    "            model_tavg.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/T2M_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\", encoding = encode_t2m)\n",
    "            model_pet.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/PET_\" +  gcm + \"_\" + ssp + \"_\" + years + \".nc\",  encoding = encode_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11187836-cf35-46ef-95c8-4f83baa401e2",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9eebf-b88f-47e8-95d7-2a23f444e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):   \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "\n",
    "            # read\n",
    "            model_tmax = xr.open_mfdataset(\"/home/rooda/Hydro_results/future_corrected/TASMAX_{}_{}*.nc\".format(gcm,ssp), chunks = \"auto\")\n",
    "            model_tavg = xr.open_mfdataset(\"/home/rooda/Hydro_results/future_corrected/T2M_{}_{}*.nc\".format(gcm,ssp),    chunks = \"auto\")\n",
    "            model_tmin = xr.open_mfdataset(\"/home/rooda/Hydro_results/future_corrected/TASMIN_{}_{}*.nc\".format(gcm,ssp), chunks = \"auto\")\n",
    "            model_pet  = xr.open_mfdataset(\"/home/rooda/Hydro_results/future_corrected/PET_{}_{}*.nc\".format(gcm,ssp),    chunks = \"auto\")\n",
    "            model_prcp = xr.open_mfdataset(\"/home/rooda/Hydro_results/future_corrected/PP_{}_{}*.nc\".format(gcm,ssp),     chunks = \"auto\")\n",
    "\n",
    "            # save\n",
    "            model_tmax.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/TASMAX_{}_{}.nc\".format(gcm,ssp), encoding = encode_tasmax)\n",
    "            model_tavg.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/T2M_{}_{}.nc\".format(gcm,ssp),    encoding = encode_t2m)\n",
    "            model_tmin.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/TASMIN_{}_{}.nc\".format(gcm,ssp), encoding = encode_tasmin)\n",
    "            model_pet.to_netcdf( \"/home/rooda/Hydro_results/future_corrected/PET_{}_{}.nc\".format(gcm,ssp),    encoding = encode_pet)\n",
    "            model_prcp.to_netcdf(\"/home/rooda/Hydro_results/future_corrected/PP_{}_{}.nc\".format(gcm,ssp),     encoding = encode_pr_alt)\n",
    "\n",
    "            files = glob.glob(\"/home/rooda/Hydro_results/future_corrected/*{}_{}_*.nc\".format(gcm,ssp))\n",
    "            for file in files: \n",
    "                os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5cb8b-9a96-4525-a5e5-78e95ef40490",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a198f23-8aec-431d-b7bc-688feb8d331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):   \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "\n",
    "            # read\n",
    "            model_tmax = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/TASMAX_{}_{}.nc\".format(gcm,ssp), chunks = \"auto\").tasmax\n",
    "            model_tavg = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/T2M_{}_{}.nc\".format(gcm,ssp),    chunks = \"auto\").t2m\n",
    "            model_tmin = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/TASMIN_{}_{}.nc\".format(gcm,ssp), chunks = \"auto\").tasmin\n",
    "            model_pet  = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/PET_{}_{}.nc\".format(gcm,ssp),    chunks = \"auto\").pet\n",
    "            model_prcp = xr.open_dataset(\"/home/rooda/Hydro_results/future_corrected/PP_{}_{}.nc\".format(gcm,ssp),     chunks = \"auto\").pr\n",
    "\n",
    "            # correct coordinates\n",
    "            assert model_tmax.lon.min() < 0\n",
    "            assert model_tavg.lon.min() < 0\n",
    "            assert model_tmin.lon.min() < 0\n",
    "            assert model_pet.lon.min()  < 0\n",
    "            assert model_prcp.lon.min() < 0\n",
    "\n",
    "            # check for na values\n",
    "            assert np.all(model_tmax.sel(lat = -45, lon = -72, method = \"nearest\")[:].data < 1e5)\n",
    "            assert np.all(model_tavg.sel(lat = -45, lon = -72, method = \"nearest\")[:].data < 1e5)\n",
    "            assert np.all(model_tmin.sel(lat = -45, lon = -72, method = \"nearest\")[:].data < 1e5)\n",
    "            assert np.all(model_pet.sel(lat = -45, lon = -72, method = \"nearest\")[:].data < 1e5)\n",
    "            assert np.all(model_prcp.sel(lat = -45, lon = -72, method = \"nearest\")[:].data < 1e5)\n",
    "\n",
    "            # \"normal\" values  \n",
    "            assert model_prcp.max().values > 10\n",
    "            assert model_tavg.mean().values > 5\n",
    "\n",
    "            # check for negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8691d0-a2c2-42b3-9d2d-44a67c144300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
