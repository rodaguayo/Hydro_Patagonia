{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc6ce84-95d4-448e-a489-dbe3c4e9cb19",
   "metadata": {},
   "source": [
    "# Glacier runoff using OGGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf4865-6547-405c-9293-8e169f3cf18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# geospatial\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# oggm\n",
    "from oggm import cfg, workflow, tasks, utils\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# basic\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "\n",
    "# import the MSsandbox modules\n",
    "from MBsandbox.mbmod_daily_oneflowline import process_pmet_data, process_gcm_data\n",
    "from MBsandbox.help_func import melt_f_calib_geod_prep_inversion\n",
    "from MBsandbox.flowline_TIModel import run_from_climate_data_TIModel, run_with_hydro_daily\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd72a8-cb10-4113-8c11-d73d35e4558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.initialize(logging_level='ERROR', future = True)\n",
    "\n",
    "cfg.PARAMS['use_multiprocessing']  = True\n",
    "cfg.PARAMS['baseline_climate']     = ''\n",
    "cfg.PARAMS['prcp_scaling_factor']  = 1\n",
    "cfg.PARAMS['hydro_month_sh']       = 1\n",
    "cfg.PARAMS['hydro_month_nh']       = 1\n",
    "cfg.PARAMS['border']               = 80\n",
    "cfg.PARAMS['geodetic_mb_period']   = '2000-01-01_2020-01-01'\n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "cfg.PARAMS['continue_on_error']    = True\n",
    "cfg.PARAMS['use_winter_prcp_factor'] = False\n",
    "cfg.PARAMS['use_rgi_area'] = False\n",
    "cfg.PARAMS['use_intersects'] = False\n",
    "cfg.PARAMS['rgi_version'] = 7\n",
    "\n",
    "param_dict = dict(t_melt  = 0, \n",
    "                  t_solid = 0,\n",
    "                  t_liq   = 2)\n",
    "\n",
    "variables = [\"volume\", \"area\", \"melt_off_glacier_daily\", \"melt_on_glacier_daily\", \"liq_prcp_off_glacier_daily\", \"liq_prcp_on_glacier_daily\"]\n",
    "encoding  = {v: {'dtype': 'float32', 'complevel' : 5, 'zlib' : True} for v in variables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469509b-c6b3-4889-af8f-b50ca2f62b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential future scenario\n",
    "gcm_list  = [\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MIROC6\", \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\"]\n",
    "ssp_list  = [\"ssp126\", \"ssp585\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bbab9-f8dd-4ae1-b124-5187f25cd3e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calibration in the reference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6594b77-4e78-45cb-ae97-678558fb2125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list with glacier geometries in RGI7 (and more data)\n",
    "ids = gpd.read_file(\"/home/rooda/OneDrive/Patagonia/GIS South/Glaciers/RGI7_Hydro.shp\")\n",
    "ids_ref_mb21 = ids.set_index(\"RGIId\").dmdtda_21\n",
    "\n",
    "cfg.PATHS['working_dir']  = \"/home/rooda/Hydro_results/oggm_runs_daily\"\n",
    "climate_file_path = \"/home/rooda/Hydro_results/PMETsim_historical_OGGM.nc\"\n",
    "\n",
    "# init glacier directories\n",
    "gdirs = workflow.init_glacier_directories(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee4db0-81c0-4d1f-8c90-fa0eba67780b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing tasks (same in \"normal\" OGGM)\n",
    "workflow.execute_entity_task(tasks.define_glacier_region, gdirs, source=\"NASADEM\");\n",
    "\n",
    "task_list = [tasks.process_dem, \n",
    "             tasks.simple_glacier_masks, \n",
    "             tasks.elevation_band_flowline,\n",
    "             tasks.fixed_dx_elevation_band_flowline, \n",
    "             tasks.compute_downstream_line,\n",
    "             tasks.compute_downstream_bedshape]\n",
    "\n",
    "for task in task_list:\n",
    "    workflow.execute_entity_task(task, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a78df-27a1-4698-b0a0-325b8fa981c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new function to process custom daily climate (at the monthly scale, it's the same)\n",
    "workflow.execute_entity_task(process_pmet_data, gdirs,\n",
    "                             temporal_resol='daily',\n",
    "                             path = climate_file_path,\n",
    "                             y0 = 1980, y1 = 2020, \n",
    "                             output_filesuffix=\"_PMET\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1ebe3-d326-4bbc-b54c-cbc2eee4d967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calibration using huggonet et al. 2021\n",
    "workflow.execute_entity_task(melt_f_calib_geod_prep_inversion, gdirs,\n",
    "                             pf=1,                    # precipitation factor\n",
    "                             mb_type=\"mb_real_daily\", # use daily temperature values\n",
    "                             grad_type=\"cte\",         # constant lapse rate (-6.5)\n",
    "                             climate_type=\"PMET\",     # climate type -> PMET\n",
    "                             ye=2020,                 # last year\n",
    "                             ref_mb=ids_ref_mb21,     # reference mb data\n",
    "                             min_melt_f = 2,          # minimum melt factor that is allowed\n",
    "                             max_melt_f = 1000,       # maximum melt factor that is allowed\n",
    "                             kwargs_for_TIModel_Sfc_Type = param_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b8ae4-f0a2-4b33-9aba-f2157a73ba9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save melt_f (not included in compile_glacier_statistics)\n",
    "melt_f_ds = pd.DataFrame()   \n",
    "for file_path in glob(cfg.PATHS['working_dir'] +  \"/*/*/*/*/melt_f*.json\", recursive = True):\n",
    "    json_data = pd.read_json(file_path, lines = True)\n",
    "    json_data[\"rgi_id\"] = file_path.split('/')[8]\n",
    "    melt_f_ds = pd.concat([melt_f_ds, json_data], ignore_index=True)\n",
    "\n",
    "melt_f_ds.to_csv(cfg.PATHS['working_dir'] + \"/glacier_statistics_melt_f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b5ed5-4a2d-4402-a2da-a3abc5ce46fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inversion by catchment (same in \"normal OGGM\")\n",
    "for zone in range(1,10):\n",
    "    ids_subset = ids[ids.ID_Zone == zone]\n",
    "    gdirs_subset = [gdir for gdir in gdirs if gdir.rgi_id in ids_subset.RGIId.tolist()]\n",
    "\n",
    "    workflow.calibrate_inversion_from_consensus(gdirs_subset, \n",
    "                                                volume_m3_reference = ids_subset.vol_M22.sum()*1e9, # Option to give an own total glacier volume to match to\n",
    "                                                apply_fs_on_mismatch=True,     # on mismatch, try to apply an arbitrary value of fs\n",
    "                                                error_on_mismatch=False,       # sometimes the given bounds do not allow to find a zero mismatch: this will normally raise an error\n",
    "                                                filter_inversion_output=True); # apply terminus thickness filtering on the inversion output\n",
    "# ready to use \n",
    "workflow.execute_entity_task(tasks.init_present_time_glacier, gdirs); \n",
    "utils.compile_glacier_statistics(gdirs);  # save statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa3fd0-61ba-40c8-a4a8-dcb2b4f70bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run\n",
    "workflow.execute_entity_task(run_with_hydro_daily, gdirs,\n",
    "                             run_task = run_from_climate_data_TIModel, # any of the `run_*`` tasks in the MBSandbox.flowline_TIModel module.\n",
    "                             ref_area_yr = 2000,                 # the hydrological output is computed over a reference area, specify here\n",
    "                             ref_area_from_y0 = False,           # overwrite ref_area_yr to the first year of the timeseries\n",
    "                             fixed_geometry_spinup_yr = 1990, \n",
    "                             ref_geometry_filesuffix = None,     # this kwarg allows to copy the reference area from a previous simulation\n",
    "                             store_annual = True,               # whether to store annual outputs or only daily outputs\n",
    "                             Testing = False,                    # if set to true, the 29th of February is set to nan values in non-leap years\n",
    "                             ys=2000, ye=2020,                   # start and end years of the model run\n",
    "                             climate_input_filesuffix=\"PMET\",    # filesuffix for the input climate file                        \n",
    "                             climate_filename='climate_historical',\n",
    "                             output_filesuffix='_historical',    # for the output file\n",
    "                             init_model_filesuffix = None,       # if you want to start from a previous model run state\n",
    "                             mb_type=\"mb_real_daily\",            # use daily temperature values\n",
    "                             grad_type=\"cte\",                    # constant lapse rate (-6.5)\n",
    "                             precipitation_factor=1,             # multiply a factor to the precipitation time series\n",
    "                             melt_f='from_json');                # calibrated melt_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d388a79-ebb7-4b34-b143-129f47f5d239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.compile_run_output(gdirs, input_filesuffix='_historical');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a35b8-8405-46ad-bd04-b08b3620f224",
   "metadata": {},
   "source": [
    "## Climate projections for each glacier and scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd91c90-b751-4863-b4ec-2e9fdb186333",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gcm in tqdm(gcm_list):    \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "        \n",
    "        rid = gcm + \"_\" + ssp\n",
    "        workflow.execute_entity_task(process_gcm_data, gdirs,\n",
    "                                     temporal_resol='daily',\n",
    "                                     path_prcp = \"/home/rooda/Hydro_results/future_corrected/PP_{}_{}.nc\".format(gcm,ssp),\n",
    "                                     path_t2m  = \"/home/rooda/Hydro_results/future_corrected/T2M_{}_{}.nc\".format(gcm,ssp),\n",
    "                                     path_hgt  = \"/home/rooda/Hydro_results/PMETsim_historical_OGGM.nc\",\n",
    "                                     y0 = 2021, y1 = 2099,  output_filesuffix= \"_daily_\" + rid);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e5cfc-7ece-46c5-b2b0-3a1f662e3299",
   "metadata": {},
   "source": [
    "## Glacier projections for each glacier and scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1068a3c-92bc-4ce6-972b-7bfa172f89e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# future projections\n",
    "for gcm in tqdm(gcm_list):    \n",
    "    for ssp in tqdm(ssp_list, leave = False):\n",
    "\n",
    "        rid = gcm + \"_\" + ssp\n",
    "\n",
    "        for zone in tqdm(range(1,10), leave = False):\n",
    "            \n",
    "            ids_subset = ids[ids.ID_Zone == zone]\n",
    "            gdirs_subset = [gdir for gdir in gdirs if gdir.rgi_id in ids_subset.RGIId.tolist()]\n",
    "            \n",
    "            workflow.execute_entity_task(run_with_hydro_daily, gdirs_subset,\n",
    "                                         run_task = run_from_climate_data_TIModel, # any of the `run_*`` tasks in the MBSandbox.flowline_TIModel module.\n",
    "                                         ref_area_from_y0 = True,                  # overwrite ref_area_yr to the first year of the timeseries\n",
    "                                         ref_geometry_filesuffix = \"_historical\",  # this kwarg allows to copy the reference area from a previous simulation\n",
    "                                         store_annual = True,                     # whether to store annual outputs or only daily outputs\n",
    "                                         Testing = False,                          # if set to true, the 29th of February is set to nan values in non-leap years\n",
    "                                         ys=2021, ye=2099,                         # start and end years of the model run\n",
    "                                         climate_input_filesuffix= rid,            # filesuffix for the input climate file                        \n",
    "                                         output_filesuffix= \"_\"+  rid,             # for the output file\n",
    "                                         init_model_filesuffix = \"_historical\",    # if you want to start from a previous model run state\n",
    "                                         climate_filename = 'gcm_data',\n",
    "                                         mb_type=\"mb_real_daily\",                  # use daily temperature values\n",
    "                                         grad_type=\"cte\",                          # constant lapse rate (-6.5)\n",
    "                                         precipitation_factor=1,                   # multiply a factor to the precipitation time series\n",
    "                                         melt_f='from_json');                      # calibrated melt_f (changed climate_type to \"PMET\")\n",
    "\n",
    "            with utils.compile_run_output(gdirs_subset, path=False, input_filesuffix = \"_\" + rid) as ds:\n",
    "                ds = ds[variables].astype(\"float32\")\n",
    "                ds.to_netcdf(cfg.PATHS['working_dir'] + \"/run_output_\" +  rid + \"_\" + str(zone) + \".nc\", encoding=encoding)\n",
    "\n",
    "            # I dont have enough space :(\n",
    "            files = glob(cfg.PATHS['working_dir'] + \"/per_glacier/*/*/*/model*\" + rid + \".nc\")\n",
    "            \n",
    "            for file in files: \n",
    "                os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fa216-4c59-4725-a727-4f5e349560e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
